

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer

np.random.seed(42)
sns.set(style="whitegrid")



def train_test_split_manual(X, y, test_ratio=0.2, seed=42):
    rng = np.random.default_rng(seed)
    n = len(X)
    idx = np.arange(n)
    rng.shuffle(idx)
    test_size = int(np.floor(test_ratio * n))
    test_idx = idx[:test_size]
    train_idx = idx[test_size:]
    return X.iloc[train_idx].reset_index(drop=True), X.iloc[test_idx].reset_index(drop=True), \
           y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)


class StandardScalerManual:
    def __init__(self):
        self.mean_ = None
        self.std_ = None

    def fit(self, X):
        X = np.asarray(X, dtype=float)
        self.mean_ = X.mean(axis=0)
        self.std_ = X.std(axis=0, ddof=0)
        self.std_[self.std_ == 0] = 1.0
        return self

    def transform(self, X):
        X = np.asarray(X, dtype=float)
        return (X - self.mean_) / self.std_

    def fit_transform(self, X):
        return self.fit(X).transform(X)


def add_bias(X):
    X = np.asarray(X, dtype=float)
    bias = np.ones((X.shape[0], 1), dtype=float)
    return np.hstack([bias, X])



def pca_svd(X, n_components=2):
    X = np.asarray(X, dtype=float)
    Xc = X - X.mean(axis=0, keepdims=True)
    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)
    components = Vt[:n_components]
    X_proj = Xc @ components.T
    return X_proj, components, X.mean(axis=0)



def sigmoid(z):
    z = np.asarray(z, dtype=float)
    out = np.empty_like(z, dtype=float)
    pos = z >= 0
    neg = ~pos
    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))
    expz = np.exp(z[neg])
    out[neg] = expz / (1.0 + expz)
    return out


def nll_loss(w, X, y, l2=0.0, reg_bias=False):
    """
    Negative log-likelihood (average) + L2 regularization
    """
    z = X @ w
    p = sigmoid(z)
    p = np.clip(p, 1e-12, 1 - 1e-12)
    ll = y * np.log(p) + (1 - y) * np.log(1 - p)
    if reg_bias:
        reg = 0.5 * l2 * (w @ w)
    else:
        reg = 0.5 * l2 * (w[1:] @ w[1:]) if w.shape[0] > 1 else 0.0
    return -np.mean(ll) + reg


def grad_nll(w, X, y, l2=0.0, reg_bias=False):
    z = X @ w
    p = sigmoid(z)
    grad = X.T @ (p - y) / X.shape[0]
    if reg_bias:
        grad += l2 * w
    else:
        if w.shape[0] > 1:
            grad = grad.copy()
            grad[1:] += l2 * w[1:]
    return grad


def hessian_nll(w, X, y, l2=0.0, reg_bias=False):
    z = X @ w
    p = sigmoid(z)
    W = p * (1 - p)  # shape (n,)
    XW = X * W[:, None]
    H = (X.T @ XW) / X.shape[0]
    if reg_bias:
        H += l2 * np.eye(X.shape[1])
    else:
        I = np.eye(X.shape[1])
        I[0, 0] = 0.0
        H += l2 * I
    return H


def batch_gd(X, y, l2=0.0, reg_bias=False, lr=0.1, decay=0.0, iters=200, w0=None,
             tol=1e-7, patience=20, verbose=False):
    w = np.zeros(X.shape[1], dtype=float) if w0 is None else w0.copy()
    history = []
    best_loss = np.inf
    wait = 0
    for t in range(iters):
        g = grad_nll(w, X, y, l2=l2, reg_bias=reg_bias)
        eta = lr / (1.0 + decay * t)
        w -= eta * g
        loss = nll_loss(w, X, y, l2=l2, reg_bias=reg_bias)
        history.append(loss)
        if verbose and t % 50 == 0:
            print(f"[BGD] iter {t:4d}, loss: {loss:.6f}, lr: {eta:.6f}")
        # early stopping
        if loss + tol < best_loss:
            best_loss = loss
            wait = 0
        else:
            wait += 1
        if wait >= patience:
            if verbose:
                print(f"[BGD] early stopping at iter {t}, loss: {loss:.6f}")
            break
    return w, np.array(history)


def iterate_minibatches(X, y, batch_size, rng):
    n = X.shape[0]
    idx = np.arange(n)
    rng.shuffle(idx)
    for start in range(0, n, batch_size):
        sl = idx[start:start + batch_size]
        yield X[sl], y[sl]


def sgd_minibatch(X, y, l2=0.0, reg_bias=False, lr=0.1, decay=0.0, iters=10, batch_size=32, seed=42, w0=None,
                  verbose=False):
    w = np.zeros(X.shape[1], dtype=float) if w0 is None else w0.copy()
    rng = np.random.default_rng(seed)
    history = []
    for t in range(iters):
        for Xb, yb in iterate_minibatches(X, y, batch_size, rng):
            g = grad_nll(w, Xb, yb, l2=l2, reg_bias=reg_bias)
            eta = lr / (1.0 + decay * (t + 1))
            w -= eta * g
        loss = nll_loss(w, X, y, l2=l2, reg_bias=reg_bias)
        history.append(loss)
        if verbose:
            print(f"[SGD] epoch {t:3d}, loss: {loss:.6f}")
    return w, np.array(history)


def newton_irls(X, y, l2=0.0, reg_bias=False, iters=20, damping=1e-6, w0=None, verbose=False):
    w = np.zeros(X.shape[1], dtype=float) if w0 is None else w0.copy()
    history = []
    for t in range(iters):
        g = grad_nll(w, X, y, l2=l2, reg_bias=reg_bias)
        H = hessian_nll(w, X, y, l2=l2, reg_bias=reg_bias)
        H_damped = H + damping * np.eye(H.shape[0])
        try:
            delta = np.linalg.solve(H_damped, g)
        except np.linalg.LinAlgError:
            delta = np.linalg.pinv(H_damped) @ g
        w -= delta
        loss = nll_loss(w, X, y, l2=l2, reg_bias=reg_bias)
        history.append(loss)
        if verbose:
            print(f"[Newton] iter {t:3d}, loss: {loss:.6f}")
    return w, np.array(history)

def predict_proba(X, w):
    return sigmoid(X @ w)


def predict_label(X, w, threshold=0.5):
    return (predict_proba(X, w) >= threshold).astype(int)


def confusion_counts(y_true, y_pred):
    y_true = y_true.astype(int)
    y_pred = y_pred.astype(int)
    TP = int(((y_true == 1) & (y_pred == 1)).sum())
    TN = int(((y_true == 0) & (y_pred == 0)).sum())
    FP = int(((y_true == 0) & (y_pred == 1)).sum())
    FN = int(((y_true == 1) & (y_pred == 0)).sum())
    return TP, TN, FP, FN


def accuracy_score(y_true, y_pred):
    TP, TN, FP, FN = confusion_counts(y_true, y_pred)
    return (TP + TN) / (TP + TN + FP + FN + 1e-12)


def precision_score(y_true, y_pred):
    TP, TN, FP, FN = confusion_counts(y_true, y_pred)
    return TP / (TP + FP + 1e-12)


def recall_score(y_true, y_pred):
    TP, TN, FP, FN = confusion_counts(y_true, y_pred)
    return TP / (TP + FN + 1e-12)


def f1_score(y_true, y_pred):
    p = precision_score(y_true, y_pred)
    r = recall_score(y_true, y_pred)
    return 2 * p * r / (p + r + 1e-12)


def roc_curve_points(y_true, y_score, num_thresh=200):
    thresholds = np.linspace(0, 1, num_thresh)
    TPR = []
    FPR = []
    for thr in thresholds:
        y_pred = (y_score >= thr).astype(int)
        TP, TN, FP, FN = confusion_counts(y_true, y_pred)
        tpr = TP / (TP + FN + 1e-12)
        fpr = FP / (FP + TN + 1e-12)
        TPR.append(tpr)
        FPR.append(fpr)
    return np.array(FPR), np.array(TPR), thresholds


def auc_trapezoid(x, y):
    order = np.argsort(x)
    x_sorted = x[order]
    y_sorted = y[order]
    return np.trapz(y_sorted, x_sorted)


def evaluate_all(X, y, w, name="Model"):
    proba = predict_proba(X, w)
    yhat = (proba >= 0.5).astype(int)
    acc = accuracy_score(y, yhat)
    prec = precision_score(y, yhat)
    rec = recall_score(y, yhat)
    f1 = f1_score(y, yhat)
    fpr, tpr, thr = roc_curve_points(y, proba, num_thresh=400)
    auc = auc_trapezoid(fpr, tpr)
    print(f"{name} -> Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}")
    return {"acc": acc, "prec": prec, "rec": rec, "f1": f1, "auc": auc, "fpr": fpr, "tpr": tpr}



def plot_convergence(histories, labels, title="Convergence (NLL Loss)"):
    plt.figure(figsize=(7, 4))
    for hist, lab in zip(histories, labels):
        xs = np.linspace(0, len(hist) - 1, len(hist))
        plt.plot(xs, hist, label=lab)
    plt.xlabel("Iteration / Epoch")
    plt.ylabel("NLL Loss")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()


def plot_roc(metrics_list, labels):
    plt.figure(figsize=(6, 5))
    for metrics, lab in zip(metrics_list, labels):
        plt.plot(metrics["fpr"], metrics["tpr"], label=f"{lab} (AUC={metrics['auc']:.3f})")
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves")
    plt.legend()
    plt.tight_layout()
    plt.show()


def pca2_decision_surface(X_train_std, y_train, X_test_std, y_test, l2=1e-2):
    # PCA on training standardized features
    Xtr_pca2, comps, tr_mean = pca_svd(X_train_std, n_components=2)
    # Center and project test using train mean and components
    Xte_c = np.asarray(X_test_std) - tr_mean
    Xte_pca2 = Xte_c @ comps.T

    # train logistic on PCA2
    Xtr2 = add_bias(Xtr_pca2)
    Xte2 = add_bias(Xte_pca2)
    w2, hist2 = batch_gd(Xtr2, y_train, l2=l2, lr=0.5, iters=1000, patience=40)

    # surface grid
    xmin, xmax = Xtr_pca2[:, 0].min() - 1, Xtr_pca2[:, 0].max() + 1
    ymin, ymax = Xtr_pca2[:, 1].min() - 1, Xtr_pca2[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 300), np.linspace(ymin, ymax, 300))
    grid = np.c_[xx.ravel(), yy.ravel()]
    grid_b = add_bias(grid)
    probs = predict_proba(grid_b, w2).reshape(xx.shape)

    plt.figure(figsize=(7, 6))
    plt.contourf(xx, yy, probs, levels=50, cmap="RdBu", alpha=0.7)
    plt.colorbar(label="P(y=1)")
    plt.scatter(Xtr_pca2[y_train == 0, 0], Xtr_pca2[y_train == 0, 1], edgecolor="k", label="malignant (0)", alpha=0.8)
    plt.scatter(Xtr_pca2[y_train == 1, 0], Xtr_pca2[y_train == 1, 1], edgecolor="k", label="benign (1)", alpha=0.8)
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title("Decision Surface (logistic trained on PCA-2)")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # evaluate PCA logistic on PCA test
    metrics_pca = evaluate_all(Xte2, y_test, w2, name="PCA-2 logistic")
    return w2, hist2, metrics_pca



def main():
    # Load dataset
    data = load_breast_cancer()
    X_full = pd.DataFrame(data.data, columns=data.feature_names)
    y_full = pd.Series(data.target, name="target")  # 0 malignant, 1 benign

    print("Dataset shape:", X_full.shape)
    print("Class counts (0=malignant,1=benign):")
    print(y_full.value_counts().sort_index().to_dict())

    # Split & scale
    X_train, X_test, y_train, y_test = train_test_split_manual(X_full, y_full, test_ratio=0.2, seed=42)
    scaler = StandardScalerManual()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.transform(X_test)

    # add bias
    Xtr = add_bias(X_train_std)
    Xte = add_bias(X_test_std)
    ytr = y_train.values.astype(float)
    yte = y_test.values.astype(float)

    # quick PCA viz
    Xtr_pca2, _, _ = pca_svd(X_train_std, n_components=2)
    plt.figure(figsize=(6, 4))
    plt.scatter(Xtr_pca2[ytr == 0, 0], Xtr_pca2[ytr == 0, 1], label="malignant", alpha=0.6)
    plt.scatter(Xtr_pca2[ytr == 1, 0], Xtr_pca2[ytr == 1, 1], label="benign", alpha=0.6)
    plt.xlabel("PC1"); plt.ylabel("PC2"); plt.title("Train PCA (2D) via SVD"); plt.legend(); plt.tight_layout()
    plt.show()

    # hyperparams
    l2 = 1e-2
    reg_bias = False

    # Train models
    print("\nTraining BGD...")
    w_bgd, hist_bgd = batch_gd(Xtr, ytr, l2=l2, reg_bias=reg_bias, lr=0.2, decay=0.001, iters=1000, patience=50, verbose=False)
    print("Training SGD...")
    w_sgd, hist_sgd = sgd_minibatch(Xtr, ytr, l2=l2, reg_bias=reg_bias, lr=0.1, decay=0.0, iters=60, batch_size=32, seed=123, verbose=False)
    print("Training Newton/IRLS...")
    w_newton, hist_newton = newton_irls(Xtr, ytr, l2=l2, reg_bias=reg_bias, iters=30, damping=1e-5, verbose=False)

    print("\nFinal train NLL losses:")
    print("BGD   :", hist_bgd[-1])
    print("SGD   :", hist_sgd[-1])
    print("Newton:", hist_newton[-1])

    # Convergence plot (align lengths for visibility)
    plot_convergence([hist_bgd, np.interp(np.linspace(0, len(hist_bgd)-1, len(hist_sgd)), np.arange(len(hist_sgd)), hist_sgd), np.interp(np.linspace(0, len(hist_bgd)-1, len(hist_newton)), np.arange(len(hist_newton)), hist_newton)],
                     ["BGD", "SGD (epoch)", "Newton"])

    # Evaluate on test set
    metrics_bgd = evaluate_all(Xte, yte, w_bgd, "BGD")
    metrics_sgd = evaluate_all(Xte, yte, w_sgd, "SGD")
    metrics_new = evaluate_all(Xte, yte, w_newton, "Newton")

    # ROC plot
    plot_roc([metrics_bgd, metrics_sgd, metrics_new], ["BGD", "SGD", "Newton"])

    # PCA-2 decision surface (train logistic on PC1/PC2 for visualization)
    print("\nPCA-2 decision surface (train separate logistic on PC scores):")
    w_pca2, hist_pca2, metrics_pca2 = pca2_decision_surface(X_train_std, ytr, X_test_std, yte, l2=l2)

    # L2 sweep (effect on weight norm and final train loss)
    l2_grid = np.logspace(-6, 1, 8)
    weight_norms = []
    train_losses = []
    for lam in l2_grid:
        w_tmp, hist_tmp = batch_gd(Xtr, ytr, l2=lam, reg_bias=False, lr=0.2, decay=0.001, iters=400, patience=60)
        weight_norms.append(np.linalg.norm(w_tmp[1:]))
        train_losses.append(hist_tmp[-1])

    plt.figure(figsize=(6, 4))
    plt.plot(l2_grid, weight_norms, marker="o")
    plt.xscale("log")
    plt.xlabel("L2 lambda"); plt.ylabel("||w|| (no bias)"); plt.title("Effect of L2 on Weight Norm"); plt.tight_layout()
    plt.show()

    plt.figure(figsize=(6, 4))
    plt.plot(l2_grid, train_losses, marker="o")
    plt.xscale("log")
    plt.xlabel("L2 lambda"); plt.ylabel("Final Train NLL"); plt.title("Effect of L2 on Training Loss"); plt.tight_layout()
    plt.show()

    print("\nFinished experiments.")


if __name__ == "__main__":
    main()
